{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# SafeVinci Fine-tuning Notebook (H100)\n",
        "\n",
        "This notebook helps you:\n",
        "- Verify GPU availability and environment\n",
        "- Prepare chat-format datasets\n",
        "- Run baseline inference with the base OmniVinci model\n",
        "- Fine-tune with QLoRA (BF16 on H100) using TRL/PEFT (optionally DeepSpeed)\n",
        "- Visualize training\n",
        "- Evaluate the fine-tuned adapter vs. baseline\n",
        "\n",
        "Prereqs:\n",
        "- Run `scripts/setup_h100.sh` once (or equivalent manual steps)\n",
        "- Base model present at `models/omnivinci/`\n",
        "- Dataset under `src/data/` with `train.jsonl`, `test.jsonl` and `annotations/`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GPU / environment checks\n",
        "import os, sys, json, subprocess, torch\n",
        "print(\"Python:\", sys.version)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"CUDA device count:\", torch.cuda.device_count())\n",
        "    print(\"GPU name:\", torch.cuda.get_device_name(0))\n",
        "    print(\"BF16 supported:\", torch.cuda.is_bf16_supported())\n",
        "    print(\"Total mem (GB):\", round(torch.cuda.get_device_properties(0).total_memory/1e9, 2))\n",
        "    !nvidia-smi || true\n",
        "else:\n",
        "    print(\"No GPU detected.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Paths and basic config\n",
        "from pathlib import Path\n",
        "PROJECT_ROOT = Path(\".\").resolve()\n",
        "MODEL_PATH = PROJECT_ROOT / \"models\" / \"omnivinci\"\n",
        "TRAIN_CHAT = PROJECT_ROOT / \"src\" / \"data\" / \"train_chat.jsonl\"\n",
        "TEST_CHAT = PROJECT_ROOT / \"src\" / \"data\" / \"test_chat.jsonl\"\n",
        "OUTPUT_DIR = PROJECT_ROOT / \"outputs\" / \"sft-omnivinci\"\n",
        "DEEPSPEED_CFG = PROJECT_ROOT / \"deepspeed_config.json\"\n",
        "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
        "print(\"MODEL_PATH:\", MODEL_PATH)\n",
        "print(\"TRAIN_CHAT exists:\", TRAIN_CHAT.exists())\n",
        "print(\"TEST_CHAT exists:\", TEST_CHAT.exists())\n",
        "print(\"OUTPUT_DIR:\", OUTPUT_DIR)\n",
        "print(\"DEEPSPEED_CFG exists:\", DEEPSPEED_CFG.exists())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: build chat datasets (safe to rerun)\n",
        "import os\n",
        "os.environ[\"PYTHONPATH\"] = str(PROJECT_ROOT)\n",
        "!python -m src.data_utils.build_sft_dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Baseline inference with base model (schema-JSON)\n",
        "import json\n",
        "from transformers import AutoProcessor, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "prompt = (\n",
        "    \"Analyze this construction safety video and return strict JSON with keys \"\n",
        "    \"incident_type, safety_status, probability, safety_response, action_plan. Reply with JSON only.\"\n",
        ")\n",
        "\n",
        "video_path = None\n",
        "# pick first test sample video\n",
        "with open(TEST_CHAT, 'r') as f:\n",
        "    line = f.readline()\n",
        "    if line:\n",
        "        item = json.loads(line)\n",
        "        video_path = item[\"conversation\"][0][\"content\"][0][\"video\"]\n",
        "\n",
        "assert video_path is not None, \"No test sample found.\"\n",
        "\n",
        "dtype = torch.bfloat16 if (torch.cuda.is_available() and torch.cuda.is_bf16_supported()) else torch.float16\n",
        "model = AutoModelForCausalLM.from_pretrained(str(MODEL_PATH), trust_remote_code=True, torch_dtype=dtype, device_map=\"auto\")\n",
        "processor = AutoProcessor.from_pretrained(str(MODEL_PATH), trust_remote_code=True)\n",
        "model.config.num_video_frames = 64\n",
        "processor.config.num_video_frames = 64\n",
        "model.config.audio_chunk_length = \"max_3600\"\n",
        "processor.config.audio_chunk_length = \"max_3600\"\n",
        "\n",
        "conversation = [{\n",
        "    \"role\": \"user\",\n",
        "    \"content\": [\n",
        "        {\"type\": \"video\", \"video\": video_path},\n",
        "        {\"type\": \"text\", \"text\": prompt}\n",
        "    ]\n",
        "}]\n",
        "text = processor.apply_chat_template(conversation, tokenize=False, add_generation_prompt=True)\n",
        "inputs = processor([text])\n",
        "out_ids = model.generate(\n",
        "    input_ids=inputs.input_ids,\n",
        "    media=getattr(inputs, 'media', None),\n",
        "    media_config=getattr(inputs, 'media_config', None),\n",
        "    max_new_tokens=256,\n",
        "    do_sample=False,\n",
        ")\n",
        "print(processor.tokenizer.batch_decode(out_ids, skip_special_tokens=True)[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Fine-tune with QLoRA (BF16 on H100) - single GPU\n",
        "import os\n",
        "os.environ[\"PYTHONPATH\"] = str(PROJECT_ROOT)\n",
        "!torchrun --standalone --nproc_per_node=1 -m src.train.train_sft --deepspeed {DEEPSPEED_CFG}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize training logs (if any)\n",
        "from pathlib import Path\n",
        "log_dir = OUTPUT_DIR\n",
        "print(\"Output dir:\", log_dir)\n",
        "# List saved files\n",
        "for p in sorted(Path(log_dir).glob(\"**/*\"))[:50]:\n",
        "    print(p)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate fine-tuned adapter vs. baseline\n",
        "import json, os\n",
        "os.environ[\"PYTHONPATH\"] = str(PROJECT_ROOT)\n",
        "\n",
        "# Evaluate base\n",
        "print(\"Evaluating base model...\")\n",
        "!python -m src.eval.infer --model_path {MODEL_PATH} --test_path {TEST_CHAT}\n",
        "\n",
        "# Evaluate adapter\n",
        "print(\"Evaluating fine-tuned adapter...\")\n",
        "!python -m src.eval.infer --model_path {OUTPUT_DIR} --base_model_path {MODEL_PATH} --test_path {TEST_CHAT}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load and preview eval predictions JSON\n",
        "import json\n",
        "from pathlib import Path\n",
        "preds_path = Path(\"outputs/eval_preds.json\")\n",
        "if preds_path.exists():\n",
        "    data = json.load(open(preds_path))\n",
        "    print(\"Num predictions:\", len(data))\n",
        "    print(json.dumps(data[:3], indent=2))\n",
        "else:\n",
        "    print(\"Predictions file not found:\", preds_path)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
